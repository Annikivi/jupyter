{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Possible solutions can be found in the [solutions.ipynb](solutions.ipynb) notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\"> *Exercise 1:* Widgets for interactive data fitting </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widgets are fun, but they can also be useful. Here's an example showing how you can fit noisy data interactively.\n",
    "\n",
    "1. Execute the cell below. It fits a 5th order polynomial to a gaussian function with some random noise \n",
    "2. Use the `@interact` decorator together with the function `fit`, such that you can visualize fits with polynomial orders `n` ranging from, say, 3 to 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaussian function\n",
    "def gauss(x,param):\n",
    "    [a,b,c] = param\n",
    "    return a*np.exp(-b*(x-c)**2)\n",
    "\n",
    "# gaussian array y in interval -5<x-5 \n",
    "nx = 100\n",
    "x = np.linspace(-5.,5.,nx)\n",
    "p = [2.0,0.5,1.5] # some parameters\n",
    "y = gauss(x,p)\n",
    "\n",
    "# add some noise\n",
    "noise = np.random.normal(0,0.2,nx)\n",
    "y += noise\n",
    "\n",
    "# we fit a 5th order polynomial to it\n",
    "\n",
    "def fit(n):\n",
    "    pfit = np.polyfit(x,y,n)\n",
    "    yfit = np.polyval(pfit,x)\n",
    "    plt.plot(x,y,\"r\",label=\"Data\")\n",
    "    plt.plot(x,yfit,\"b\",label=\"Fit\")\n",
    "    plt.legend()\n",
    "    plt.ylim(-0.5,2.5)\n",
    "    plt.show()\n",
    "    \n",
    "# call function fit\n",
    "# these lines are unnecessary when you use the interact widget\n",
    "n=5\n",
    "fit(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\"> *Exercise 2a:* Cell profiling </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise is about cell profiling, but you will get practice in working with magics and cells.\n",
    "\n",
    "1. Load the random_walk.py code (in the current directory) into a cell below with the appropriate magic command \n",
    "    - note that you have to rerun the cell after the content is loaded\n",
    "2. Split up the functions over cells (either via Edit menu or keyboard shortcut `Ctrl-Shift-minus`). \n",
    "3. Initializating `n` and calling `walk()` doesn't need to be in a main function, and you can remove the `__name__` stuff.\n",
    "4. Plot the random walk trajectory.\n",
    "5. Time the execution of `walk()` with a line magic.\n",
    "6. Run the prun cell profiler.\n",
    "7. Can you spot a little mistake which is slowing down the code?\n",
    "8. In the next exercise you will install a line profiler which will more easily expose the performance mistake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\"> *Exercise 2b:* Installing a magic command for line profiling </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Magics can be installed using `pip` and loaded like plugins using the `%load_ext` magic. You will now install a line-profiler to get more detailed profile, and hopefully find insight to speed up the code from the previous exercise.\n",
    "\n",
    "1. First install the line profiler using `!pip install line_profiler`.\n",
    "2. Next load it using `%load_ext line_profiler`.\n",
    "3. Have a look at the new magic command that has been enabled with `%lprun?`\n",
    "3. Load the `random_walk.py` into a new cell, and execute it.\n",
    "4. In a new cells, run the line profiler on each function of the example code using something like:   \n",
    "`%lprun -f <func1> -f <func2> -f <func3> main()`\n",
    "5. Inspect the output. Can you more easily see the mistake now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\"> *Exercise 3:* Data analysis with pandas dataframes </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data science and data analysis are key use cases of Jupyter. In this exercise you will familiarize yourself with dataframes and various inbuilt analysis methods in the high-level `pandas` data exploration library. A dataset containing information on Nobel prizes will be used.\n",
    "\n",
    "1. Start by navigating in the File Browser to the `data/` subfolder, and double-click on the `nobels.csv` dataset. This will open JupyterLab's inbuilt data browser.\n",
    "2. Have a look at the data, column names, etc.\n",
    "3. In a your own notebook, import the `pandas` module and load the dataset into a *dataframe*:  \n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "nobel = pd.read_csv(\"data/nobels.csv\")\n",
    "```\n",
    "\n",
    "4. The \"share\" column of the dataframe contains the number of Nobel recipients that shared the prize. Have a look at the statistics of this column using  \n",
    "\n",
    "```python\n",
    "nobels[\"share\"].describe()\n",
    "```\n",
    "\n",
    "5. The `describe()` method is smart about data types. Try this:  \n",
    "```python\n",
    "nobel[\"bornCountryCode\"].describe()\n",
    "```\n",
    "\n",
    "    - What country has received the largest number of Nobel prizes, and how many?\n",
    "    - How many countries are represented in the dataset?\n",
    "6. Now analyze the age of prize recipients. You first need to convert the \"born\" column to datetime format: \n",
    "\n",
    "```python\n",
    "nobel[\"born\"] = pd.to_datetime(nobel[\"born\"], \n",
    "                               errors ='coerce')\n",
    "```\n",
    "\n",
    "7. Next subtract the birth date from the year of receiving the prize and insert it into a new column \"age\":\n",
    "```python\n",
    "nobel[\"age\"] = nobel[\"year\"] - nobel[\"born\"].dt.year\n",
    "nobel[[\"surname\",\"age\"]].head(10)\n",
    "```\n",
    " - Now print the \"surname\" and \"age\" of first 10 entries using the `head()` method.\n",
    "\n",
    "8. Now plot results in two different ways:\n",
    "\n",
    "```python\n",
    "nobel[\"age\"].plot.hist(bins=[20,30,40,50,60,70,80,\n",
    "                             90,100],alpha=0.6);\n",
    "nobel.boxplot(column=\"age\", by=\"category\")\n",
    "```\n",
    "\n",
    "9. Which Nobel laureates have been Swedish? See if you can use the `nobel.loc[CONDITION]` statement to extract the relevant rows from the `nobel` dataframe using the appropriate condition.\n",
    "\n",
    "10. Finally, try the powerful `groupby()` method to analyze the number of Nobel prizes per country, and visualize it with the high-level `seaborn` plotting library. \n",
    " - First add a column \"number\" to the `nobel` dataframe containing 1's (to enable the counting below).\n",
    " - Then extract any 4 countries (replace below) and create a subset of the dataframe:\n",
    "```python\n",
    "countries = np.array([COUNTRY1, COUNTRY2, COUNTRY3, COUNTRY4])\n",
    "nobel2 = nobel.loc[nobel['bornCountry'].isin(countries)]\n",
    "```\n",
    " - Next use `groupby()` and `sum()`, and inspect the resulting dataframe:\n",
    "```python\n",
    "nobels_by_country = nobel2.groupby(['bornCountry',\"category\"], \n",
    "                                   sort=True).sum()\n",
    "```\n",
    " - Next use the `pivot_table` method to reshape the dataframe to a spreadsheet-like structure, and display the result:\n",
    "```python\n",
    "table = nobel2.pivot_table(values=\"number\", index=\"bornCountry\", \n",
    "                           columns=\"category\", aggfunc=np.sum)\n",
    "```\n",
    " - Finally visualize using a heatmap:\n",
    " ```python\n",
    "import seaborn as sns\n",
    "sns.heatmap(table,linewidths=.5);\n",
    "```\n",
    "    - Have a look at the help page for `sns.heatmap` and see if you can find an input parameter which annotates each cell in the plot with the count number.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\"> *Exercise 4:* Defining your own custom magic command </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to create new magic commands using the `@register_cell_magic` decorator from the `IPython.core` library. Here you will create a cell magic command that compiles C++ code and executes it.\n",
    "\n",
    "\n",
    "> This example has been adapted from the [IPython Minibook](http://ipython-books.github.io/), by Cyrille Rossant, Packt Publishing, 2015.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First import `register_cell_magic`\n",
    "\n",
    "```python\n",
    "from IPython.core.magic import register_cell_magic\n",
    "```\n",
    "\n",
    "2. Next execute the cell below here to register the new cell magic command. You can now start using the magic using `%%cpp`.\n",
    "\n",
    "3. Write some C++ code into a cell and try executing it.\n",
    "\n",
    "4. To be able to use the magic in another notebook, you need to add the following function at the end and then write the cell to a file in your PYTHONPATH. If the file is called `cpp_ext.py`, you can then load it by `%load_ext cpp_ext`.\n",
    "\n",
    "```python\n",
    "def load_ipython_extension(ipython):\n",
    "    ipython.register_magic_function(cpp,'cell')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_cell_magic\n",
    "def cpp(line, cell):\n",
    "    \"\"\"Compile, execute C++ code, and return the standard output.\"\"\"\n",
    "\n",
    "    # We first retrieve the current IPython interpreter instance.\n",
    "    ip = get_ipython()\n",
    "    # We define the source and executable filenames.\n",
    "    source_filename = '_temp.cpp'\n",
    "    program_filename = '_temp'\n",
    "    # We write the code to the C++ file.\n",
    "    with open(source_filename, 'w') as f:\n",
    "        f.write(cell)\n",
    "    # We compile the C++ code into an executable.\n",
    "    compile = ip.getoutput(\"g++ {0:s} -o {1:s}\".format(\n",
    "        source_filename, program_filename))\n",
    "    # We execute the executable and return the output.\n",
    "    output = ip.getoutput('./{0:s}'.format(program_filename))\n",
    "    print('\\n'.join(output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\"> *Exercise 5:* Parallel Python with ipyparallel </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditionally, Python is considered to not support parallel programming very well ([see \"GIL\"](https://en.wikipedia.org/wiki/Global_interpreter_lock)), and \"proper\" parallel programming should be left to \"heavy-duty\" languages like Fortran or C/C++ where OpenMP and MPI can be utilised. \n",
    "\n",
    "However, IPython now supports many different styles of parallelism which can be useful to researchers. In particular, `ipyparallel` enables all types of parallel applications to be developed, executed, debugged, and monitored interactively. Possible use cases of `ipyparallel` include:\n",
    "- Quickly parallelize algorithms that are embarrassingly parallel using a number of simple approaches.\n",
    "- Run a set of tasks on a set of CPUs using dynamic load balancing.\n",
    "- Develop, test and debug new parallel algorithms (that may use MPI) interactively.\n",
    "- Analyze and visualize large datasets (that could be remote and/or distributed) interactively using IPython\n",
    "\n",
    "This exercise is just to get started, for a thorough treatment see the [official documentation](https://ipyparallel.readthedocs.io/en/latest/) and [this detailed tutorial](https://github.com/DaanVanHauwermeiren/ipyparallel-tutorial).\n",
    "\n",
    "1. First install `ipyparallel` using `conda` or `pip`. Open a terminal window inside JupyterLab and do the installation.\n",
    "2. After installing `ipyparallel`, you need to start an \"IPython cluster\". Do this in the terminal with `ipcluster start`.\n",
    "3. Then import `ipyparallel` in your notebook, initialize a `Client` instance, and create *DirectView* object for direct execution on the engines:\n",
    "```python\n",
    "import ipyparallel as ipp\n",
    "client = ipp.Client()\n",
    "print(\"Number of ipyparallel engines:\", len(client.ids))\n",
    "dview = client[:]\n",
    "```\n",
    "4. You have now started the parallel engines. To run something simple on each one of them, try the `apply_sync()` method:\n",
    "```python\n",
    "cluster[:].apply_sync(lambda : \"Hello, World\")\n",
    "```\n",
    "5. A serial evaluation of squares of integers can be seen in the code snippet below. \n",
    "```python\n",
    "serial_result = list(map(lambda x:x**2, range(30)))\n",
    "```\n",
    " - Convert this to a parallel calculation on the engines using the `map_sync()` method of the DirectView instance. Time both serial and parallel versions using `%%timeit -n 1`.\n",
    "\n",
    "6. You will now parallelize the evaluation of $\\pi$ using a Monte Carlo method. First load modules, and export the `random` module to the engines:\n",
    "```python\n",
    "from random import random\n",
    "from math import pi\n",
    "dview['random'] = random\n",
    "```\n",
    "Then execute the following code in a cell. The function `mcpi` is a Monte Carlo method to calculate $\\pi$. Time the execution of this function using `%timeit -n 1` and a sample size of 10 million (`int(1e7)`).\n",
    "```python\n",
    "def mcpi(nsamples):\n",
    "    s = 0\n",
    "    for i in range(nsamples):\n",
    "        x = random()\n",
    "        y = random()\n",
    "        if x*x + y*y <= 1:\n",
    "            s+=1\n",
    "    return 4.*s/nsamples\n",
    "```    \n",
    "Now take the incomplete function below which takes a `DirectView` object and a number of samples, divides the number of samples between the engines, and calls `mcpi()` with a subset of the samples on each engine. Complete the function (by replacing the `____` fields), call it with $10^7$ samples, time it and compare with the serial call to `mcpi()`.\n",
    "```python\n",
    "def multi_mcpi(dview, nsamples):\n",
    "    # get total number target engines\n",
    "    p = len(____.targets)\n",
    "    if nsamples % p:\n",
    "        # ensure even divisibility\n",
    "        nsamples += p - (nsamples%p)\n",
    "    \n",
    "    subsamples = ____//p\n",
    "    \n",
    "    ar = view.apply(mcpi, ____)\n",
    "    return sum(ar)/____\n",
    "```\n",
    "\n",
    "Final note: While parallelizing Python code is often worth it, there are other ways to get higher performance out of Python code. In particular, fast numerical packages like [Numpy](http://www.numpy.org/) should be used, and significant speedup can be obtained with just-in-time compilation with [Numba](https://numba.pydata.org/) and/or C-extensions from [Cython](http://cython.org/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (JupLab)",
   "language": "python",
   "name": "juplab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
